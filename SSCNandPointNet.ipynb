{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6210481d-56de-4748-8010-533a0670766f",
   "metadata": {},
   "source": [
    "# SSCN and PointNet Layer Implementation Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9f27b-189f-4565-8dab-9004535c4426",
   "metadata": {},
   "source": [
    "SSCN uses the preprocessing method used in [Volumetric and Multi-View CNNs for Object Classification on 3D Data](https://arxiv.org/abs/1604.03265)\n",
    "\n",
    "We are going to use a simpler method that doesn't require another CNN, where we voxelize the given .off files and project them from multiple angles to get a 2d image\n",
    "we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4485d5-b95c-41ad-803e-ed6c98c32062",
   "metadata": {},
   "source": [
    "### Download binvox and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c6e26e-4907-456f-97ea-dc0002ecee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "!test -f binvox || wget https://www.patrickmin.com/binvox/linux64/binvox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e756be5e-cc93-4fa9-abb7-11b940aa918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x binvox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbd0bf55-a5c6-482a-8c66-7570c525ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./download_dataset.sh > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6283b5-d91d-46f2-a4ac-b7ad7924fb68",
   "metadata": {},
   "source": [
    "### Runs script to voxelize `.off` file\n",
    "`/dev/null` pipe is to stop it from breaking stdout, if the program is not working, remove it to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08afc5e3-4341-498b-8606-7e1fd8f78fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./voxelize.sh ModelNet10/bed/train/bed_0010.off > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1a46b2-8351-49ad-bcf0-aaa065d913c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -std=c++11 read_vox.cpp -o read_vox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5986f046-42d0-405b-a35b-07753230d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading binvox version 1\n",
      "  read 143417 voxels\n",
      "Writing voxel data to ASCII file...\n",
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!./read_vox ModelNet10/bed/train/bed_0010.binvox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0895f6-f17b-4696-8520-0b6664837bba",
   "metadata": {},
   "source": [
    "# Parse voxel file into C++ memory\n",
    "This is for SSCN since we need 2d Projected 3d objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0cf8d6-b46c-433b-a8a4-6005e9de63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include<fstream>\n",
    "#include<iostream>\n",
    "#include<vector>\n",
    "#include<string>\n",
    "using namespace std;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "167cb7e8-d278-4ab2-add2-4b4a6088730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef vector<vector<vector<char>>> v3d;\n",
    "typedef vector<vector<char>> v2d;\n",
    "typedef vector<vector<float>> v2df;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218ef33-8c71-4be1-a7f7-c49adc1e8622",
   "metadata": {},
   "source": [
    "Read all voxel data from voxels.txt and output in 3d vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecaf3004-2003-4fca-8e62-d0bb16ae00e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "v3d read_vox(){\n",
    "    ifstream voxels(\"voxels.txt\");\n",
    "    string t;\n",
    "    int x, y, z;\n",
    "    // ignore irrelevant \n",
    "    voxels >> t >> t >> t >> t;\n",
    "    voxels >> x >> y >> z;\n",
    "    voxels >> t >> t >> t >> t;\n",
    "    voxels >> t >> t >> t;\n",
    "    vector<vector<vector<char>>> data(x, vector<vector<char>>(z, vector<char>(y, 0)));\n",
    "    // Read data into vec\n",
    "    // format is [x][z][y]\n",
    "    for(int i = 0; i < x; i++){\n",
    "        for(int j = 0; j < z; j++){\n",
    "            for(int k = 0; k < y; k++){\n",
    "                char v;\n",
    "                voxels >> v;\n",
    "                data[i][j][k] = v;\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    }\n",
    "    return data;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2c04c6-6610-4e42-9fcf-c168bdac3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "char max(char x, char y) {\n",
    "    return x > y ? x : y;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f34c644-a0c0-42cd-9e73-7a14f542c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Project onto x y or z\n",
    "// sweeps across given axis\n",
    "// 0 = x, 1 = z, 2 = y\n",
    "v2d project(const v3d &data, int axis, int x_size, int y_size){\n",
    "    v2d out(x_size, vector<char>(y_size, 0));\n",
    "    if(axis == 0){\n",
    "        for(int i = 0; i < data.size(); i++){\n",
    "            for(int j = 0; j < data[0].size(); j++){\n",
    "                for(int k = 0; k < data[0][0].size(); k++){\n",
    "                    out[j][k] = max(out[j][k], data[i][j][k]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    } else if(axis == 1){\n",
    "        for(int i = 0; i < data.size(); i++){\n",
    "            for(int j = 0; j < data[0].size(); j++){\n",
    "                for(int k = 0; k < data[0][0].size(); k++){\n",
    "                    out[j][k] = max(out[j][k], data[j][i][k]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    } else{\n",
    "        for(int i = 0; i < data.size(); i++){\n",
    "            for(int j = 0; j < data[0].size(); j++){\n",
    "                for(int k = 0; k < data[0][0].size(); k++){\n",
    "                    out[j][k] = max(out[j][k], data[j][k][i]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return out;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9127c2c1-4abd-4edf-aa2d-fb6aae1584d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "void print_2d(const v2d &data){\n",
    "    for(auto i : data){\n",
    "        for (auto j : i){\n",
    "            cout << j << \" \";\n",
    "        }\n",
    "        cout << endl;\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e84e3ca-b23c-4cad-9c2d-f836ce40b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "void print_2df(const v2df &data){\n",
    "    for(auto i : data){\n",
    "        for (auto j : i){\n",
    "            cout << j << \"\\t\";\n",
    "        }\n",
    "        cout << endl;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d987a9-6469-4337-96d6-e66e5edb345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 256\n",
      "y: 256\n",
      "z: 256\n"
     ]
    }
   ],
   "source": [
    "// Get our 3d vector of intputs\n",
    "v3d data = read_vox();\n",
    "int x = data.size();\n",
    "int z = data[0].size();\n",
    "int y = data[0][0].size();\n",
    "cout << \"x: \" << x << endl;\n",
    "cout << \"y: \" << y << endl;\n",
    "cout << \"z: \" << z << endl;\n",
    "v2d p = project(data, 2, x, y);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755922cd-1622-4876-aad6-3cb0af252800",
   "metadata": {},
   "source": [
    "## SCNN Assignment\n",
    "The goal of this assignment will be to write a 2D sparse convolution. Sparse convolutions are essentially the same as normal convolutions, but values in the center of a kernel will remain zero to prevent exploding of sparse data. \n",
    "\n",
    "For simplicity, your kernel will be 5x5, and it will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccf64e77-28d5-4e6f-971a-a7a73564fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Convert byte array to floats\n",
    "typedef vector<vector<float>> v2df;\n",
    "v2df pf;\n",
    "for(int i = 0; i < x; i ++){\n",
    "    vector<float> v;\n",
    "    for(int j = 0; j < y; j++){\n",
    "        v.push_back((float) p[i][j]);\n",
    "    }\n",
    "    pf.push_back(v);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaaa398c-9d04-4dbb-a0dd-02c52555cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "//ACTION: Complete this sparse convolution implementation\n",
    "v2df sparse_convolution(const v2df& data, const v2df& kernel){\n",
    "    // Dimensions of input data\n",
    "    int dataHeight = data.size();\n",
    "    int dataWidth = data[0].size();\n",
    "    \n",
    "    // Dimensions of kernel (fixed to 5x5)\n",
    "    const int kernelSize = 5;\n",
    "    const int pad = kernelSize / 2;\n",
    "\n",
    "    // Output dimensions\n",
    "    int outputHeight = dataHeight;\n",
    "    int outputWidth = dataWidth;\n",
    "\n",
    "    // Initialize output with zeros\n",
    "    v2df output(outputHeight, vector<float>(outputWidth, 0.0f));\n",
    "\n",
    "    // Perform 2D convolution\n",
    "    // REMOVE THIS FOR STUDENT TO FILL IN FOR ASSIGNMENT\n",
    "    for (int i = 0; i < dataHeight; ++i) {\n",
    "        for (int j = 0; j < dataWidth; ++j) {\n",
    "            // Accumulate convolution sum\n",
    "            float sum = 0.0f;\n",
    "            if (data[i][j] != 0){\n",
    "                for (int ki = 0; ki < kernelSize; ++ki) {\n",
    "                    for (int kj = 0; kj < kernelSize; ++kj) {\n",
    "                        // Determine input indices\n",
    "                        int ni = i + ki - pad;\n",
    "                        int nj = j + kj - pad;\n",
    "\n",
    "                        // Check for boundary conditions\n",
    "                        if (ni >= 0 && ni < dataHeight && nj >= 0 && nj < dataWidth) {\n",
    "                            sum += data[ni][nj] * kernel[ki][kj];\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            // Store result in output\n",
    "                output[i][j] = sum;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return output;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f99fb8e7-804a-40d3-852a-783967d8c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "v2df kernel;\n",
    "for(int i = 0; i < 5; i++){\n",
    "    vector<float> v;\n",
    "    for(int j = 0; j < 5; j++){\n",
    "        v.push_back(i + j);\n",
    "    }\n",
    "    kernel.push_back(v);\n",
    "}\n",
    "// Running on layer\n",
    "v2df result = sparse_convolution(pf, kernel);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c7eb5-54e6-4f22-a6d2-8b2eefa44770",
   "metadata": {},
   "source": [
    "### Line example\n",
    "If your convolution is working properly, this line example should still produce2 intersection lines after convolution instead of exploding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58450936-c592-43a2-a417-95dafd3574e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\t0\t0.36\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.48\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t1.68\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t1.68\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0.72\t1.44\t1.68\t1.68\t1.68\t1.68\t1.2\t1.2\t1.2\t1.2\t1.2\t1.2\t1.2\t1.2\t1.2\t1.2\t1.2\t1.2\t0.96\t0.72\t\n",
      "0\t0\t0\t1.68\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t1.68\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.6\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.48\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "0\t0\t0\t0.36\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
      "row sum: 24.96\n",
      "col sum: 16.68\n"
     ]
    }
   ],
   "source": [
    "v2df cross;\n",
    "for(int i = 0; i < 20; i++){\n",
    "    vector<float> v;\n",
    "    for(int j = 0; j < 20; j++){\n",
    "        if (i == 5){\n",
    "            v.push_back(0.24);\n",
    "\n",
    "        } \n",
    "        else if(j == 3){\n",
    "            v.push_back(0.12);\n",
    "        }\n",
    "        else{\n",
    "            v.push_back(0);\n",
    "        }\n",
    "    }\n",
    "    cross.push_back(v);\n",
    "}\n",
    "\n",
    "v2df k;\n",
    "for(int i = 0; i < 5; i++){\n",
    "    vector<float> v;\n",
    "    for(int j = 0; j < 5; j++){\n",
    "        v.push_back(1);\n",
    "    }\n",
    "    k.push_back(v);\n",
    "}\n",
    "v2df cross_res = sparse_convolution(cross, k);\n",
    "print_2df(cross_res);\n",
    "\n",
    "// Verification number\n",
    "float sum = 0;\n",
    "for(int i = 0; i < 20; i++){\n",
    "    sum += cross_res[5][i];\n",
    "}\n",
    "cout << \"row sum: \" << sum << endl;\n",
    "sum = 0;\n",
    "for(int i = 0; i < 20; i++){\n",
    "    sum += cross_res[i][3];\n",
    "}\n",
    "cout << \"col sum: \" << sum << endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055272e3-299b-4f47-b61f-91c8a4d4e241",
   "metadata": {},
   "source": [
    "### Expected Results\n",
    "row sum: 24.96\n",
    "\n",
    "col sum: 16.68\n",
    "\n",
    "In the output grid, all value not along row 5 or column 3 should be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2d711-5f3a-4fb3-aca3-2f1bc8d93bee",
   "metadata": {},
   "source": [
    "# PointNet++ Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db530828-6a20-414e-9999-b2f3eeea2894",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "These are functions that are implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e97d706-265b-4a70-af98-adad5cd15a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <iostream>\n",
    "#include <fstream>\n",
    "#include <vector>\n",
    "#include <cmath>\n",
    "#include <limits>\n",
    "#include <algorithm>\n",
    "#include <random>\n",
    "using namespace std;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3cab7ea-6ec6-44b9-b2b8-b5f1a8d89c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Generate randome numbers for weights\n",
    "float random_float(float min, float max) {\n",
    "    int seed = 1;\n",
    "    mt19937 gen(seed);\n",
    "    uniform_real_distribution<> dis(min, max);\n",
    "    return dis(gen);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cea3abd-1da6-418f-a359-81d9a787b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Activation function: ReLU\n",
    "float relu(float x) {\n",
    "    return max(0.0f, x);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faadfe5c-e9f4-4a8a-a45d-334ede5f7fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "float dot_product(const vector<float>& p1, const vector<float>& p2) {\n",
    "    return p1[0] * p2[0] + p1[1] * p2[1] + p1[2] * p2[2];\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b6e2047-586c-429a-a905-620a815c8fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Matrix-vector multiplication\n",
    "auto mat_vec_mul(const vector<vector<float>>& mat, const vector<float>& vec) {\n",
    "    vector<float> result(mat.size(), 0.0f);\n",
    "    for (size_t i = 0; i < mat.size(); ++i) {\n",
    "        for (size_t j = 0; j < mat[0].size(); ++j) {\n",
    "            result[i] += mat[i][j] * vec[j];\n",
    "        }\n",
    "    }\n",
    "    return result;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23a40ae2-95ff-4861-8148-b39c71575000",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Add bias vector to output\n",
    "auto add_bias(const vector<float>& vec, const vector<float>& bias) {\n",
    "    vector<float> result = vec;\n",
    "    for (size_t i = 0; i < vec.size(); ++i) {\n",
    "        result[i] += bias[i];\n",
    "    }\n",
    "    return result;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da81924-fe1d-487a-8f8b-af1870e3e710",
   "metadata": {},
   "source": [
    "### Point Distance\n",
    "Fill in the function `euclidean_distance` to find the distance between two 3D points\n",
    "HINT:\n",
    "$$\n",
    "p_1 = (x_1, y_1, z_1)\n",
    "$$\n",
    "$$\n",
    "p_2 = (x_2, y_2, z_2)\n",
    "$$\n",
    "$$\n",
    "d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdc01528-4d4c-4f18-9787-3f96dd8ecb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "// ACTION: Write a function to find the distance between two points\n",
    "float euclidean_distance(const vector<float>& p1, const vector<float>& p2) {\n",
    "    // DELETE THIS PART IN FINAL ASSIGNMENT\n",
    "    return sqrt(pow(p1[0] - p2[0], 2) + pow(p1[1] - p2[1], 2) + pow(p1[2] - p2[2], 2));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412e260-1af7-4faa-b266-d50bc26c1e64",
   "metadata": {},
   "source": [
    "### Farthest Point Sampling\n",
    "This is a sampling technique where given points $\\{ x_1, x_2, \\dots, x_n\\}$, we need to find a subset of these points $\\{x_{i_1},x_{i_2}, \\dots, x_{i_m}\\}$ where $x_{i_j}$ is the furthest point from the set $\\{x_{i_1},x_{i_2}, \\dots, x_{j-1}\\}$. We use this technique to ensure an even coverage points across the whole pointcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a5bad68-e236-4b6f-90de-0bc7e5889cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "// ACTION: Implement farthest point sampling\n",
    "// returns group of vectors, vector<float>\n",
    "auto farthest_point_sampling(const vector<vector<float>>& points, int num_samples) {\n",
    "    vector<int> sampled_indices;\n",
    "    int n = points.size();\n",
    "    vector<float> distances(n, numeric_limits<float>::infinity());\n",
    "\n",
    "    // Randomly select the first point\n",
    "    sampled_indices.push_back(rand() % n);\n",
    "\n",
    "    // DELETE THIS PART IN FINAL ASSIGNMENT\n",
    "    for (int i = 1; i < num_samples; ++i) {\n",
    "        float max_dist = -1;\n",
    "        int farthest_point = -1;\n",
    "\n",
    "        // Compute distances from the sampled points\n",
    "        for (int j = 0; j < n; ++j) {\n",
    "            float min_dist = numeric_limits<float>::infinity();\n",
    "            for (int k : sampled_indices) {\n",
    "                float dist = euclidean_distance(points[j], points[k]);\n",
    "                min_dist = min(min_dist, dist);\n",
    "            }\n",
    "            distances[j] = min_dist;\n",
    "            if (distances[j] > max_dist) {\n",
    "                max_dist = distances[j];\n",
    "                farthest_point = j;\n",
    "            }\n",
    "        }\n",
    "        sampled_indices.push_back(farthest_point);\n",
    "    }\n",
    "    return sampled_indices;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe09a82b-6185-4957-8add-d1896a214c9f",
   "metadata": {},
   "source": [
    "### Group Neighbors\n",
    "Group points into neighborhoods based on radius. We input points, centroids and a radius. We group all of the points around the centroids, and those groups around the centroids get added to our grouped_points variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd69679d-2eea-4956-8c5c-bb9e89c71eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "// ACTION: Fill in the function to group points around the centroids within the radius.\n",
    "auto group_neighbors(const vector<vector<float>>& points, const vector<vector<float>>& centroids, float radius) {\n",
    "    vector<vector<vector<float>>> grouped_points;\n",
    "    \n",
    "    // DELETE THIS PART IN FINAL ASSIGNMENT\n",
    "    for (const auto& centroid : centroids) {\n",
    "        vector<vector<float>> neighbors;\n",
    "        for (const auto& point : points) {\n",
    "            if (euclidean_distance(centroid, point) <= radius) {\n",
    "                neighbors.push_back(point);\n",
    "            }\n",
    "        }\n",
    "        grouped_points.push_back(neighbors);\n",
    "    }\n",
    "    return grouped_points;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afb841-d911-415b-a018-63c93198b7e9",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron (MLP)\n",
    "Now we need to put all of these functions together. We will be randomly generating weights, since we won't be implementing backpropagation to update weights in this assignment. \n",
    "Our MLP has the following steps\n",
    "1. Aggregate point features with average pooling\n",
    "2. Perform matrix vector multiplication on aggregated point features and weights\n",
    "3. Add biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a9c97b0-f588-4f24-896c-2e2861c14e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// MLP Function\n",
    "// Note: we do not learn our weights and biases here and just use random values\n",
    "// ACTION: fill in the action blocks of this function\n",
    "auto mlp(const vector<vector<float>>& points, int input_dim, int output_dim) {\n",
    "    // Number of points in the group\n",
    "    size_t num_points = points.size();\n",
    "\n",
    "    // Random number generator for weights and biases\n",
    "    int seed = 1;\n",
    "    mt19937 gen(seed);\n",
    "    uniform_real_distribution<> dis(-0.1, 0.1);\n",
    "\n",
    "    // Initialize weights and biases for a single-layer MLP\n",
    "    vector<vector<float>> weights(output_dim, vector<float>(input_dim, 0.0f));\n",
    "    vector<float> biases(output_dim, 0.0f);\n",
    "\n",
    "    // Fill weights and biases with random values\n",
    "    for (size_t i = 0; i < output_dim; ++i) {\n",
    "        for (size_t j = 0; j < input_dim; ++j) {\n",
    "            weights[i][j] = dis(gen);\n",
    "        }\n",
    "        biases[i] = dis(gen);\n",
    "    }\n",
    "\n",
    "    // Aggregate point features (e.g., average pooling)\n",
    "    // This is the sum portion of the average pooling\n",
    "    vector<float> aggregated_features(input_dim, 0.0f);\n",
    "    for (const auto& point : points) {\n",
    "        // ACTION: Fill in the interior of this loop\n",
    "        for (int i = 0; i < input_dim; ++i) {\n",
    "            // DELETE THIS PART FOR ASSIGNMENT RELEASE\n",
    "            aggregated_features[i] += point[i];\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Normalize by the number of points\n",
    "    // This is the division portion of the average pooling\n",
    "    // ACTION: Fill in the interior of this loop\n",
    "    for (int i = 0; i < input_dim; ++i) {\n",
    "        \n",
    "        // DELETE THIS PART FOR ASSIGNMENT RELEASE\n",
    "        aggregated_features[i] /= num_points; \n",
    "    }\n",
    "\n",
    "    // Apply MLP: matrix-vector multiplication + bias addition + activation\n",
    "    // ACTION: Perform matrix vector multiplication\n",
    "    vector<float> output = mat_vec_mul(weights, aggregated_features); // REMOVE RIGHT SIDE OF EQUALS FOR RELEASE\n",
    "    output = add_bias(output, biases); // keep this part\n",
    "\n",
    "    // Apply ReLU activation\n",
    "    for (auto& val : output) {\n",
    "        val = relu(val);\n",
    "    }\n",
    "\n",
    "    return output; // Return the processed feature vector\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf78a8-311c-47f0-b9d5-4ce448adb774",
   "metadata": {},
   "source": [
    "### Max Pooling\n",
    "Perform max pooling on a list of features. This will be the maximum of all the dimensions, so max pooling of $\\{(1, 2, 3), (3, 2, 1)\\} = (3, 2, 3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "398f770f-c8fe-4457-aa7e-8191e2db12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Max pooling operation\n",
    "// ACTION: Implement Max pooling\n",
    "auto max_pool(const vector<vector<float>>& features) {\n",
    "    vector<float> pooled(features[0].size(), -numeric_limits<float>::infinity());\n",
    "    // DELETE THIS PART FOR ASSIGNMENT RELEASE\n",
    "    for (const auto& feature : features) {\n",
    "        for (size_t i = 0; i < feature.size(); ++i) {\n",
    "            pooled[i] = max(pooled[i], feature[i]);\n",
    "        }\n",
    "    }\n",
    "    return pooled;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a5d04f-de1d-4bac-9373-16c19875276a",
   "metadata": {},
   "source": [
    "## PointNet++ Forward Pass\n",
    "This portion will implement forward pass using the functions you implemented earlier. Here's the steps:\n",
    "1. Obtain sample points (using FPS)\n",
    "2. Use the indexes of these sampled points to obtain the centroids\n",
    "3. Group the points around the centroids (sampled_points) with group_neighbors\n",
    "4. Process each region with MLP\n",
    "5. Perform max pooling over the features from MLP\n",
    "6. Update features for next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84029de6-1429-416c-a2ed-6b4a8426aa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "// PointNet++ Core Algorithm\n",
    "auto pointnet_plus_plus(const vector<vector<float>>& input_points, int num_layers, int k_neighbors, float radius) {\n",
    "    vector<vector<float>> current_points = input_points;\n",
    "    vector<float> final_features;\n",
    "\n",
    "    // Iterate through the layers\n",
    "    for (int layer = 0; layer < num_layers; ++layer) {\n",
    "        // Sample centroids using Farthest Point Sampling (FPS)\n",
    "        vector<int> sampled_indices = farthest_point_sampling(current_points, k_neighbors);\n",
    "\n",
    "        // Extract centroids\n",
    "        vector<vector<float>> sampled_points;\n",
    "        for (int idx : sampled_indices) {\n",
    "            sampled_points.push_back(current_points[idx]);\n",
    "        }\n",
    "\n",
    "        // Group neighboring points for each centroid\n",
    "        vector<vector<vector<float>>> grouped_points = group_neighbors(current_points, sampled_points, radius);\n",
    "\n",
    "        // Process each local region using an MLP (PointNet subroutine)\n",
    "        vector<vector<float>> local_features;\n",
    "        for (const auto& group : grouped_points) {\n",
    "            local_features.push_back(mlp(group, 3, 128)); // Input 3D points, output feature size 128\n",
    "        }\n",
    "\n",
    "        // Max pooling over local features\n",
    "        vector<float> aggregated_features = max_pool(local_features);\n",
    "\n",
    "        // Update current points with aggregated features for the next layer\n",
    "        current_points.clear();\n",
    "        current_points.push_back(aggregated_features);  // Simplified: use aggregated feature as the next layer's input\n",
    "    }\n",
    "\n",
    "    // Final feature aggregation via max pooling\n",
    "    final_features = max_pool(current_points);\n",
    "\n",
    "    return final_features;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04e979-5af2-452d-b62d-9a3246c75f31",
   "metadata": {},
   "source": [
    "## PointNet++ Testing\n",
    "Next we will test your implementation of the functions. We will load in points from bed_0001 in ModelNet10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "becaffa1-68a8-4e99-9105-4d1d12e43944",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto load_points (){\n",
    "    ifstream pstream(\"ModelNet10/bed/train/bed_0001.off\");\n",
    "    string off;\n",
    "    int num_points, surface, mesh;\n",
    "    vector<vector<float>> points;\n",
    "    pstream >> off >> num_points >> surface >> mesh;\n",
    "    for(int i = 0; i < num_points; i++){\n",
    "        vector<float> p;\n",
    "        for(int j = 0; j < 3; j++){\n",
    "            float dim;\n",
    "            pstream >> dim;\n",
    "            p.push_back(dim);\n",
    "        }\n",
    "        points.push_back(p);\n",
    "    }\n",
    "    return points;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5d534dd-3f8e-40aa-b342-e4389e4b0fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CHECK_SUM: 3.05936"
     ]
    }
   ],
   "source": [
    "vector<vector<float>> points = load_points();\n",
    "vector<float> final_features = pointnet_plus_plus(points, 5, 100, 100);\n",
    "float sum = 0;\n",
    "for(auto feature : final_features){\n",
    "    sum += feature;\n",
    "}\n",
    "cout << \"\\nCHECK_SUM: \" << sum;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b05e1f-a9ff-4ccb-ad32-d3b2b50ab7d7",
   "metadata": {},
   "source": [
    "The expected value for CHECK_SUM on first pass is 3.05936"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
